{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers evaluate"
      ],
      "metadata": {
        "id": "R1r0Det0KLEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8VapGsHf94Oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "-T7QF30CVY3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer,TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "mWDiBceY88YO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIc9sQzY0Qd2"
      },
      "outputs": [],
      "source": [
        "model_id=\"FacebookAI/roberta-base\"\n",
        "num_labels=28\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_id, num_labels=num_labels, problem_type=\"multi_label_classification\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"go_emotions\")\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "wv80jq5M9bmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=dataset[\"train\"]\n",
        "eval_dataset=dataset[\"validation\"]\n",
        "#print(train_dataset[\"id\"][0])\n",
        "labels = train_dataset.features[\"labels\"].feature.names\n",
        "print({i: l for i, l in enumerate(labels)})\n"
      ],
      "metadata": {
        "id": "qVp601cGK_Vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "bwlZJeYuR_2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Get label names\n",
        "labels = train_dataset.features[\"labels\"].feature.names\n",
        "\n",
        "# Count occurrences of each label\n",
        "label_counts = Counter()\n",
        "\n",
        "for sample in train_dataset[\"labels\"]:\n",
        "    for label_idx in sample:  # Each sample has multiple labels\n",
        "        label_counts[label_idx] += 1\n",
        "\n",
        "# Convert to a dictionary with label names\n",
        "label_distribution = {labels[i]: count for i, count in label_counts.items()}\n",
        "\n",
        "# Print the distribution\n",
        "for emotion, count in sorted(label_distribution.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{emotion}: {count}\")\n"
      ],
      "metadata": {
        "id": "SPWxNnA1Ouc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Get the index of \"neutral\"\n",
        "neutral_idx = labels.index(\"neutral\")\n",
        "\n",
        "# Identify sentences where \"neutral\" is the only label\n",
        "purely_neutral = [i for i in range(len(train_dataset)) if train_dataset[i][\"labels\"] == [neutral_idx]]\n",
        "\n",
        "# Identify sentences where \"neutral\" appears with other emotions\n",
        "neutral_with_other = [i for i in range(len(train_dataset)) if neutral_idx in train_dataset[i][\"labels\"] and len(train_dataset[i][\"labels\"]) > 1]\n",
        "\n",
        "print(f\"Total purely neutral sentences: {len(purely_neutral)}\")\n",
        "print(f\"Total neutral + other emotion sentences: {len(neutral_with_other)}\")\n",
        "\n",
        "# Decide how much of the purely neutral sentences to remove (e.g., keep 25%)\n",
        "remove_fraction = 0.5  # Adjust based on dataset balance\n",
        "num_to_remove = int(len(purely_neutral) * remove_fraction)\n",
        "\n",
        "# Randomly sample the indices to remove\n",
        "to_remove = random.sample(purely_neutral, num_to_remove)\n",
        "\n",
        "# Create a new dataset without the selected purely neutral sentences\n",
        "filtered_train_dataset = [\n",
        "    train_dataset[i] for i in range(len(train_dataset))\n",
        "    if i not in to_remove  # Keep reduced neutral-only\n",
        "]\n",
        "\n",
        "print(f\"New dataset size: {len(filtered_train_dataset)}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vKDXQGiTIWuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_train_dataset[1]"
      ],
      "metadata": {
        "id": "hbhlNgmEVn20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from datasets import Dataset, ClassLabel, Sequence, Features, Value\n",
        "\n",
        "# Convert the filtered list back to a Hugging Face Dataset\n",
        "# and explicitly specify the features using the Features class\n",
        "features = Features({\n",
        "    \"text\": Value(\"string\"),  # Change \"string\" to Value(\"string\")\n",
        "    \"labels\": Sequence(ClassLabel(names=dataset[\"train\"].features[\"labels\"].feature.names)),\n",
        "    \"id\": Value(\"string\")   # Change \"string\" to Value(\"string\n",
        "}) # Use Features class to define the schema\n",
        "filtered_train_dataset = Dataset.from_list(filtered_train_dataset, features=features)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BLJsaJKhUMid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(filtered_train_dataset)"
      ],
      "metadata": {
        "id": "HyHMegmePmKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_train_dataset[0][\"labels\"]"
      ],
      "metadata": {
        "id": "LahkNhyrUq9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Get label names\n",
        "labels = filtered_train_dataset.features[\"labels\"].feature.names\n",
        "\n",
        "# Count occurrences of each label\n",
        "label_counts = Counter()\n",
        "\n",
        "for sample in filtered_train_dataset[\"labels\"]:\n",
        "    for label_idx in sample:  # Each sample has multiple labels\n",
        "        label_counts[label_idx] += 1\n",
        "\n",
        "# Convert to a dictionary with label names\n",
        "label_distribution = {labels[i]: count for i, count in label_counts.items()}\n",
        "\n",
        "# Print the distribution\n",
        "for emotion, count in sorted(label_distribution.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{emotion}: {count}\")\n"
      ],
      "metadata": {
        "id": "gH8SuO5oVhqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotions = [\"grief\",\"pride\",\"relief\",\"nervousness\",\"embarrassment\",\"remorse\"\n",
        ",\"fear\",\"desire\",\"disgust\",\"excitement\"]\n",
        "for sample in emotions:\n",
        "    emo_idx = labels.index(sample)\n",
        "\n",
        "    # Identify sentences where \"neutral\" is the only label\n",
        "    print(sample)\n",
        "    print(label_distribution[sample])\n",
        "    purely_emo = [i for i in range(len(filtered_train_dataset)) if filtered_train_dataset[i][\"labels\"] == [emo_idx]]\n",
        "    print(len(purely_emo))"
      ],
      "metadata": {
        "id": "kOytp0kbY1Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X = filtered_train_dataset[\"text\"]  # Extract input text\n",
        "Y = filtered_train_dataset[\"labels\"]  # Extract labels\n",
        "\n",
        "\n",
        "num_labels = len(labels)  # Total number of emotions\n",
        "\n",
        "# Convert list of label indices into multi-hot encoding\n",
        "Y_multi_hot = np.zeros((len(Y), num_labels))\n",
        "for i, label_list in enumerate(Y):\n",
        "    Y_multi_hot[i, label_list] = 1  # Set 1 for each emotion present\n",
        "\n",
        "print(Y_multi_hot.shape)  # Should be (num_samples, num_labels)\n",
        "print(num_labels)\n",
        "print(Y_multi_hot[0])\n",
        "print(train_dataset[0][\"lables\"])"
      ],
      "metadata": {
        "id": "nX31ZXXsWqfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y_multi_hot[1040])\n",
        "print(filtered_train_dataset[1040][\"labels\"])"
      ],
      "metadata": {
        "id": "2_-AUZ10Wapa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count occurrences of each label\n",
        "label_counts = np.sum(Y_multi_hot, axis=0)  # Sum along all samples\n",
        "total_samples = len(Y_multi_hot)\n",
        "\n",
        "# Compute class weights as (neg/pos) for each label\n",
        "pos = label_counts\n",
        "neg = total_samples - pos\n",
        "class_weights = neg / pos\n",
        "\n",
        "# Handle division by zero (if any label has zero positives)\n",
        "class_weights = np.nan_to_num(class_weights, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "# Convert to tensor\n",
        "#class_weights = torch.tensor(class_weights, dtype=torch.float32).to(\"cuda\")\n",
        "\n",
        "print(label_counts)\n",
        "print(pos)\n",
        "print(neg)\n",
        "print(class_weights)"
      ],
      "metadata": {
        "id": "3H9AIkYDLOPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_labels(example):\n",
        "    multi_hot = np.zeros(num_labels, dtype=np.float32)  # Use float32\n",
        "    for label in example[\"labels\"]:\n",
        "        multi_hot[label] = 1.0  # Assign as float\n",
        "    example[\"labels\"] = multi_hot.tolist()\n",
        "    example.pop(\"id\", None)\n",
        "    return example\n",
        "\n",
        "filtered_train_dataset = filtered_train_dataset.map(encode_labels)\n",
        "eval_dataset = eval_dataset.map(encode_labels)"
      ],
      "metadata": {
        "id": "MivqDR0PX-qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "tokenized_train_dataset = filtered_train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n"
      ],
      "metadata": {
        "id": "lY_daI7UXg9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        # Ensure labels are float and on the correct device\n",
        "        labels = inputs.get(\"labels\").float().to(model.device)\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Use pos_weight instead of weight\n",
        "        loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights.to(model.device))  # Move class_weights to device\n",
        "\n",
        "        # Explicitly cast logits to float32 (if necessary)\n",
        "        loss = loss_fn(logits.float(), labels)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "bHBPsl7jI8lE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"f1\")  # F1-score\n",
        "precision_metric = evaluate.load(\"precision\")\n",
        "recall_metric = evaluate.load(\"recall\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred  # Unpack predictions and labels\n",
        "\n",
        "    # Convert logits to probabilities\n",
        "    probs = torch.sigmoid(torch.tensor(logits))\n",
        "\n",
        "    # Apply threshold to get binary predictions (0 or 1)\n",
        "    preds = (probs > 0.5).int().numpy()\n",
        "    labels = np.array(labels, dtype=np.int32)\n",
        "    preds = preds.reshape(-1)  # Flatten predictions\n",
        "    labels = labels.reshape(-1)\n",
        "    # Compute F1, Precision, and Recall\n",
        "    f1 = metric.compute(predictions=preds, references=labels, average=\"macro\")\n",
        "    precision = precision_metric.compute(predictions=preds, references=labels, average=\"macro\")\n",
        "    recall = recall_metric.compute(predictions=preds, references=labels, average=\"macro\")\n",
        "\n",
        "    return {\n",
        "        \"f1\": f1[\"f1\"],\n",
        "        \"precision\": precision[\"precision\"],\n",
        "        \"recall\": recall[\"recall\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "mCLfDzhrJGb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_eval_dataset[0][\"labels\"])  # Should be a multi-hot list like [0, 1, 0, ...]"
      ],
      "metadata": {
        "id": "Oj4fIgmNGKnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/llm-finetuning\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=8,   # Reduce batch size to save memory\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=3e-5,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,  # Use mixed precision training to save memory\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=500,\n",
        "    save_total_limit=2,  # Keep only the last 2 checkpoints\n",
        "    load_best_model_at_end=True,  # Load best model based on eval loss\n",
        ")\n",
        "\n",
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_eval_dataset,\n",
        "    compute_metrics=compute_metrics,  # Add evaluation function\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "kv3K0KSP921r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load trained model\n",
        "classifier = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_all_scores=True,\n",
        "    top_k=None,\n",
        "    id2label={i: label for i, label in enumerate(labels)}  # Map indices to emotion names\n",
        ")\n",
        "\n",
        "# Test sentence\n",
        "sentence = \"I'm so excited for the weekend!\"\n",
        "\n",
        "# Get predictions\n",
        "preds = classifier(sentence)\n",
        "\n",
        "# Print results\n",
        "for p in preds[0]:\n",
        "    print(f\"{p['label']}: {p['score']:.4f}\")\n"
      ],
      "metadata": {
        "id": "T5_18wA0L_Fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run evaluation on test dataset\n",
        "results = trainer.evaluate()\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "RrrkA7pfMAz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = tokenizer(\"I love this movie!\", return_tensors=\"pt\")\n",
        "print(tokenizer.decode(sample[\"input_ids\"][0]))  # Should reconstruct the original text"
      ],
      "metadata": {
        "id": "JAYer3M7AaNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = [\n",
        "    \"I love this movie!\",\n",
        "    \"I'm so annoyed with my internet connection.\",\n",
        "    \"This is amazing!\",\n",
        "    \"I'm feeling really sad today.\",\n",
        "    \"I feel nervous about my exam.\"\n",
        "]\n",
        "\n",
        "# Get predictions for multiple sentences\n",
        "batch_preds = classifier(test_sentences)\n",
        "\n",
        "# Display results\n",
        "for i, sentence in enumerate(test_sentences):\n",
        "    print(f\"\\nSentence: {sentence}\")\n",
        "    for p in batch_preds[i]:\n",
        "        print(f\"  {p['label']}: {p['score']:.4f}\")\n"
      ],
      "metadata": {
        "id": "ecJOdRmVMHeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_path = \"/content/drive/MyDrive/llm-finetuning\"  # Change if needed\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Load your trained model and tokenizerh\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Define emotion labels (GoEmotions has 28)\n",
        "emotion_labels = [\"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\", \"caring\", \"confusion\",\n",
        "                  \"curiosity\", \"desire\", \"disappointment\", \"disapproval\", \"disgust\", \"embarrassment\",\n",
        "                  \"excitement\", \"fear\", \"gratitude\", \"grief\", \"joy\", \"love\", \"nervousness\", \"optimism\",\n",
        "                  \"pride\", \"realization\", \"relief\", \"remorse\", \"sadness\", \"surprise\", \"neutral\"]\n",
        "\n",
        "# Function to predict emotions\n",
        "def predict_emotions(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.sigmoid(logits)  # Use softmax if it's multi-class classification\n",
        "    predicted_emotions = [emotion_labels[i] for i, p in enumerate(probabilities[0]) if p > 0.5]  # Threshold = 0.5\n",
        "\n",
        "    return predicted_emotions\n",
        "\n",
        "# Example sentence\n",
        "text = \"I am really happy today!\"\n",
        "predicted_emotions = predict_emotions(text)\n",
        "print(\"Predicted Emotions:\", predicted_emotions)\n"
      ],
      "metadata": {
        "id": "9CMz_H0m4Kuv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}